\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}

\title{Eigenvalue Calculation}
\author{AI24BTECH11020 - Rishika Kotha}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Eigenvalue calculation is a fundamental problem in linear algebra with many applications in physics, engineering, and computer science. Given a square matrix \( A \), the goal is to find scalar values \( \lambda \) (eigenvalues) and corresponding vectors \( \mathbf{v} \) (eigenvectors) such that:

\[
A \mathbf{v} = \lambda \mathbf{v}
\]

 we implement and analyze the \textbf{QR algorithm}, which computes the eigenvalues of a matrix through repeated QR decompositions. The QR algorithm is chosen for its numerical stability and general applicability to various types of matrices.

\section{QR Algorithm}

The \textbf{QR algorithm} is an iterative method for computing the eigenvalues of a matrix. It involves decomposing the matrix \( A \) into an orthogonal matrix \( Q \) and an upper triangular matrix \( R \) such that:
\[
A = QR
\]

The process is repeated by reconstructing a new matrix \( A' = RQ \) from the product of \( R \) and \( Q \). With each iteration, the matrix \( A' \) approaches an upper triangular form, and the eigenvalues become the diagonal elements of the matrix after sufficient iterations.

\subsection{QR Algorithm: Basic Steps}
The QR algorithm follows these basic steps:
\begin{enumerate}
    \item Start with an initial matrix \( A_0 = A \).
    \item Perform the QR decomposition of \( A_n = Q_n R_n \).
    \item Reconstruct the new matrix \( A_{n+1} = R_n Q_n \).
    \item Repeat the process until convergence (i.e., when the off-diagonal elements of the matrix become negligible).
    \item The eigenvalues of the original matrix \( A \) are approximated by the diagonal elements of the final matrix \( A_n \).
\end{enumerate}

The QR algorithm works well for general matrices and is especially efficient for all types of matrices.
\subsection{Gram-Schmidt Process}
 \textbf{Gram-Schmidt process} is used to do \textbf{QR-decomposition} by orthogonalizing the columns of \( A \)\\
\textbf{Steps}:
\begin{enumerate}
    \item Let \( A = [a_1, a_2, \dots, a_n] \), where \( a_i \) are the column vectors of \( A \).
    \item Initialize:
    \[
    q_1 = \frac{a_1}{\|a_1\|}
    \]
    \item For \( i = 2, \dots, n \):
    \begin{enumerate}
        \item Compute:
        \[
        q_i = a_i - \text{proj}_{q_1}(a_i) - \text{proj}_{q_2}(a_i) - \dots - \text{proj}_{q_{i-1}}(a_i)
        \]
        where
        \[
        \text{proj}_{q}(a) = \frac{q^T a}{q^T q} q
        \]
        \item Normalize:
        \[
        q_i = \frac{q_i}{\|q_i\|}
        \]
    \end{enumerate}
    \item Construct:
    \[
    Q = [q_1, q_2, \dots, q_n], \quad R = Q^T A
    \]
    we get $Q$,an orthogonal matrix and 
    $R$,an upper triangular matrix.
\end{enumerate}
\section{Time Complexity Analysis}

The time complexity of the QR algorithm is dominated by the cost of performing the QR decomposition at each iteration. The QR decomposition of an \( n \times n \) matrix takes \( O(n^3) \) operations. If we denote the number of iterations required for convergence as \( k \), the overall time complexity of the algorithm is:

\[O(k \cdot n^3)\]

In practice, the number of iterations($k$) required for convergence depends on the properties of the matrix, such as its condition number and spectral properties. For well-conditioned matrices, the QR algorithm typically converges within \( O(n) \) iterations. However, in the worst case, it may take more iterations.

\section{Other Insights}

\subsection{Memory Usage}
The QR algorithm requires storing multiple matrices during each iteration. Specifically, for an \( n \times n \) matrix, we need to store:\\
- The original matrix \( A \) and the reconstructed matrices \( A_{n+1} \).\\
- The orthogonal matrix \( Q \) and the upper triangular matrix \( R \).

Each of these matrices requires \( O(n^2) \) memory. Therefore, the memory usage for the QR algorithm is \( O(n^2) \).

\subsection{Convergence Rate}
The convergence rate of the QR algorithm depends on the matrix's spectral properties. For matrices with distinct eigenvalues, the QR algorithm converges rapidly. However, for matrices with clustered eigenvalues, convergence can be slower. In practice, the algorithm is quite efficient for many real-world matrices, especially when implemented with optimized linear algebra libraries.

\subsection{Suitability for Different Types of Matrices}
- \textbf{Symmetric Matrices}: The QR algorithm is highly effective for symmetric matrices, as it will quickly converge to the eigenvalues. The orthogonal matrix \( Q \) in each step retains the symmetry of the matrix.\\
- \textbf{Sparse Matrices}: For sparse matrices, the QR algorithm is not inherently efficient, as it requires dense matrix operations. Specialized methods, such as the Lanczos algorithm, may be more suitable for sparse matrices.\\
- \textbf{Non-Symmetric Matrices}: The QR algorithm is well-suited for non-symmetric matrices, as it works for both real and complex eigenvalues.

\section{Comparison of Algorithms}

In addition to the QR algorithm, several other algorithms are commonly used for eigenvalue computation, including:

\subsection{ Power Iteration}
\textbf{Principle:}
Power Iteration is an iterative method to estimate the largest eigenvalue (in magnitude) and its corresponding eigenvector. It repeatedly multiplies a vector \(v\) by the matrix \(A\), normalizing the result:
\[
v_{k+1} = \frac{Av_k}{\|Av_k\|}.
\]

\textbf{Characteristics}
\begin{itemize}
    \item \textbf{Applicability:} General matrices (not limited to symmetric ones).
    \item \textbf{Output:} Dominant eigenvalue and eigenvector.
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item Simple and easy to implement.
    \item Computationally efficient for sparse matrices.
    \item Requires only matrix-vector multiplication.
\end{itemize}

\textbf{Limitations}
\begin{itemize}
    \item Only computes the largest eigenvalue.
    \item Convergence slows if eigenvalues are close in magnitude.
    \item May fail for defective or poorly conditioned matrices.
\end{itemize}

\subsection{ Jacobi Method}

\textbf{Principle}
The Jacobi Method is an iterative algorithm for symmetric matrices. It reduces the matrix to diagonal form through a sequence of orthogonal similarity transformations:
\[
A' = R^TAR.
\]

\textbf{Characteristics}
\begin{itemize}
    \item \textbf{Applicability:} Symmetric matrices only.
    \item \textbf{Output:} All eigenvalues and eigenvectors.
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item Numerically stable due to orthogonal transformations.
    \item Converges to exact solutions for symmetric matrices.
    \item Simple to implement and parallelizable.
\end{itemize}

\textbf{Limitations}
\begin{itemize}
    \item Computationally expensive for large matrices (\(O(n^3)\)).
    \item Slower convergence compared to modern algorithms.
    \item Limited to symmetric matrices.
\end{itemize}

\subsection{Divide-and-Conquer Method}

\textbf{Principle}
The Divide-and-Conquer algorithm efficiently computes eigenvalues of symmetric matrices by recursively dividing a tridiagonal matrix into smaller submatrices, computing their eigenvalues, and merging the results.

\textbf{Algorithm}
\begin{enumerate}
    \item Transform the matrix to tridiagonal form.
    \item Divide the tridiagonal matrix into smaller submatrices.
    \item Compute eigenvalues and eigenvectors for the submatrices.
    \item Merge the results recursively.
\end{enumerate}

\textbf{Characteristics}
\begin{itemize}
    \item \textbf{Applicability:} Symmetric matrices, especially dense ones.
    \item \textbf{Output:} All eigenvalues and eigenvectors.
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item Highly efficient for dense symmetric matrices.
    \item Faster than the QR algorithm for large problems.
    \item Suitable for parallel and high-performance computing.
\end{itemize}

\textbf{Limitations}
\begin{itemize}
    \item Requires preprocessing to convert the matrix to tridiagonal form.
    \item Complex to implement compared to other methods.
    \item Overkill for small matrices.
\end{itemize}
\section{Recommendations}
\begin{itemize}
    \item \textbf{Power Iteration:} Use when only the largest eigenvalue is needed, especially for sparse matrices.
    \item \textbf{Jacobi Method:} Best for small symmetric matrices where simplicity and high accuracy are required.
    \item \textbf{Divide-and-Conquer:} Ideal for large symmetric matrices in high-performance computing.
\end{itemize}

\subsection{Comparison in Terms of Time Complexity}
\begin{itemize}
    \item \textbf{QR Algorithm}: \( O(k \cdot n^3) \), where \( k \) is the number of iterations.
    \item \textbf{Power Iteration}: \( O(n^2) \) for each iteration, but only works for the largest eigenvalue.
    \item \textbf{Jacobi Method}: \( O(n^3) \), but it is more suitable for symmetric matrices.
    \item \textbf{Divide and Conquer}: \( O(n^3) \) for symmetric matrices.
\end{itemize}

\subsection{Comparison in Terms of Accuracy}
- The \textbf{QR algorithm} is accurate and converges to the exact eigenvalues, provided that the matrix is not ill-conditioned.\\
- The \textbf{Power Iteration method} can fail to converge to all eigenvalues, especially when they are clustered or have similar magnitudes.\\
- The \textbf{Jacobi Method} is accurate for symmetric matrices but can be slow for large matrices.\\
- The \textbf{Divide and Conquer} method is accurate for symmetric matrices and is more efficient than the QR algorithm for very large matrices.

\subsection{Suitability for Specific Types of Matrices}
- \textbf{QR Algorithm}: Works well for general, non-symmetric matrices and is the method of choice for large-scale eigenvalue problems.\\
- \textbf{Power Iteration} : Best suited for finding the dominant eigenvalue of large, sparse matrices.\\
-\textbf{Jacobi Method} : Suitable for small, symmetric matrices where accuracy is important.\\
- \textbf{Divide and Conquer}: Most effective for symmetric matrices, especially when the matrix is large.

\section{Conclusion}

The QR algorithm is an effective and general-purpose method for computing the eigenvalues of a matrix. It is well-suited for non-symmetric matrices and large-scale problems. The time complexity of the QR algorithm is \( O(k \cdot n^3) \), where \( k \) is the number of iterations, and its memory usage is \( O(n^2) \). The method is numerically stable and converges efficiently for most matrices. However, for specialized matrices, alternative algorithms such as \textbf{Power Iteration} or \textbf{Jacobi Method} may be more appropriate depending on the matrix's properties.

\end{document}
e
